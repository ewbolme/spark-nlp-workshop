{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1uRgJ5MzqoGh-XYQUFAVBUGQYlLi7aMXx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ABSA_glove_absa.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Start Spark session\n",
    "spark = sparknlp.start(gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"xx\")\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "    \n",
    "loaded_ner_model = NerDLModel.load(\"ABSA_glove_absa\")\\\n",
    "    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"absa\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"document\", \"token\", \"absa\"])\\\n",
    "    .setOutputCol(\"absa_span\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        token,\n",
    "        glove_embeddings,\n",
    "        loaded_ner_model,\n",
    "        converter])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "prediction_model = ner_prediction_pipeline.fit(empty_data)\n",
    "sent_pipeline = Pipeline(\n",
    "    stages = [document, sentence]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = os.listdir('../inputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [os.path.join('../inputs/', file) for file in input_files]\n",
    "output_paths = [os.path.join('../outputs/', file.replace('txt', 'csv')) for file in input_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for in_path, out_path in zip(input_paths, output_paths):\n",
    "    text = open(in_path).read()\n",
    "    df = spark.createDataFrame(pd.DataFrame({'text': [text]}))\n",
    "    df1 = prediction_model.transform(df).toPandas()\n",
    "    df2 = sent_pipeline.fit(empty_data).transform(df).toPandas()\n",
    "\n",
    "    all_sents = df2['sentence'][0]\n",
    "\n",
    "    sentences = []\n",
    "    aspects = []\n",
    "    sentiments = []\n",
    "    for result in df1['absa_span'][0]:\n",
    "        start, end = result['begin'], result['end']\n",
    "        for sent in all_sents:\n",
    "            if sent['begin'] <= start and sent['end'] >= end:\n",
    "                sentences.append(sent['result'])\n",
    "        aspects.append(result['result'])\n",
    "        sentiment = \"positive\" if result['metadata']['entity'] == \"POS\" else \"negative\"\n",
    "        sentiments.append(sentiment)\n",
    "    final_result = pd.DataFrame.from_dict({\"sentence\": sentences, \"aspect\": aspects, \"sentiment\": sentiments})\n",
    "    final_result.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/Example10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
