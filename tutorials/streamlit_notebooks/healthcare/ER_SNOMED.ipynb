{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA21Jo5d9SVq"
   },
   "source": [
    "\n",
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/ER_SNOMED.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzIdjHkAW8TB"
   },
   "source": [
    "# **SNOMED coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uDmeHEFW7_h"
   },
   "source": [
    "To run this yourself, you will need to upload your license keys to the notebook. Otherwise, you can look at the example outputs at the bottom of the notebook. To upload license keys, open the file explorer on the left side of the screen and upload `workshop_license_keys.json` to the folder that opens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIeCOiJNW-88"
   },
   "source": [
    "## 1. Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMIDv74CYN0d"
   },
   "source": [
    "Import license keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1540,
     "status": "ok",
     "timestamp": 1601203278696,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "ttHPIV2JXbIM",
    "outputId": "56d1dab9-c4ea-4999-b011-721cc36c8fed"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/content/spark_nlp_for_healthcare.json', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n",
    "\n",
    "secret = license_keys['SECRET']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "sparknlp_version = license_keys[\"PUBLIC_VERSION\"]\n",
    "jsl_version = license_keys[\"JSL_VERSION\"]\n",
    "\n",
    "print ('SparkNLP Version:', sparknlp_version)\n",
    "print ('SparkNLP-JSL Version:', jsl_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQtc1CHaYQjU"
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 76775,
     "status": "ok",
     "timestamp": 1601203356669,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "CGJktFHdHL1n",
    "outputId": "2c22c5f8-85b3-4f79-a5c0-34e133b8e85a"
   },
   "outputs": [],
   "source": [
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==$sparknlp_version\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version --extra-index-url https://pypi.johnsnowlabs.com/$secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj5FRDV4YSXN"
   },
   "source": [
    "Import dependencies into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUWyj8c6JSPP"
   },
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed6Htm7qDQB3"
   },
   "source": [
    "Start the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaSM8-xhDRa4"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RgiqfX5XDqb"
   },
   "source": [
    "## 2. Select the Entity Resolver model and construct the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVKr8C2SrkZQ"
   },
   "source": [
    "Select the models:\n",
    "\n",
    "\n",
    "* SNOMED Entity Resolver models: **chunkresolve_snomed_findings_clinical**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For more details: https://github.com/JohnSnowLabs/spark-nlp-models#pretrained-models---spark-nlp-for-healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK9xxkkfrsLc"
   },
   "outputs": [],
   "source": [
    "# Change this to the model you want to use and re-run the cells below.\n",
    "ER_MODEL_NAME = \"chunkresolve_snomed_findings_clinical\"\n",
    "NER_MODEL_NAME = \"ner_clinical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zweiG2ilZqoR"
   },
   "source": [
    "Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "executionInfo": {
     "elapsed": 245320,
     "status": "ok",
     "timestamp": 1601204391067,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LLuDz_t40be4",
    "outputId": "7f2060f5-4b51-44d5-be7e-f7276ae60cdc"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences']) \\\n",
    "    .setOutputCol('tokens')\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = NerDLModel().pretrained(NER_MODEL_NAME, 'en', 'clinical/models')\\\n",
    "    .setInputCols(\"sentences\", \"tokens\", \"embeddings\")\\\n",
    "    .setOutputCol(\"ner_tags\")   \n",
    "\n",
    "# filtering ner output by whitelisting\n",
    "ner_chunker = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"ner_chunk\").setWhiteList(['PROBLEM','TREATMENT'])\n",
    "\n",
    "chunk_embeddings = ChunkEmbeddings()\\\n",
    "    .setInputCols(\"ner_chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"chunk_embeddings\")\n",
    "\n",
    "entity_resolver = \\\n",
    "    ChunkEntityResolverModel.pretrained(ER_MODEL_NAME,\"en\",\"clinical/models\")\\\n",
    "    .setInputCols(\"tokens\",\"chunk_embeddings\").setOutputCol(\"resolution\")\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_chunker,\n",
    "    chunk_embeddings,\n",
    "    entity_resolver])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "light_pipeline = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y9GpdJhXIpD"
   },
   "source": [
    "## 3. Create example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBOKkB2THdGI"
   },
   "outputs": [],
   "source": [
    "# Enter examples as strings in this array\n",
    "input_list = [\n",
    "\"\"\"She is followed by Dr. X in our office and has a history of severe tricuspid regurgitation with mild elevation and PA pressure. On 05/12/08, preserved left and right ventricular systolic function, aortic sclerosis with apparent mild aortic stenosis, and bi-atrial enlargement. She has previously had a Persantine Myoview nuclear rest-stress test scan completed at ABCD Medical Center in 07/06 that was negative. She has had significant mitral valve regurgitation in the past being moderate, but on the most recent echocardiogram on 05/12/08, that was not felt to be significant. She has a history of hypertension and EKGs in our office show normal sinus rhythm with frequent APCs versus wandering atrial pacemaker. She does have a history of significant hypertension in the past. She has had dizzy spells and denies clearly any true syncope. She has had bradycardia in the past from beta-blocker therapy.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gmrjqHSGcJx"
   },
   "source": [
    "# 4. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdhgKutMHUoC"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame({\"text\": input_list}))\n",
    "result = pipeline_model.transform(df)\n",
    "light_result = light_pipeline.fullAnnotate(input_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIVShVLhI68M"
   },
   "source": [
    "# 5. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472iBPpK-FvF"
   },
   "source": [
    "Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 9760,
     "status": "ok",
     "timestamp": 1601204598935,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "Qdh2BQaLI7tU",
    "outputId": "a81f2601-bacb-4eac-fb36-d4f006ca69c2"
   },
   "outputs": [],
   "source": [
    "result.select(\n",
    "    F.explode(\n",
    "        F.arrays_zip('ner_chunk.result', \n",
    "                     'ner_chunk.begin',\n",
    "                     'ner_chunk.end',\n",
    "                     'ner_chunk.metadata',\n",
    "                     'resolution.metadata', 'resolution.result')\n",
    "    ).alias('cols')\n",
    ").select(\n",
    "    F.expr(\"cols['0']\").alias('chunk'),\n",
    "    F.expr(\"cols['1']\").alias('begin'),\n",
    "    F.expr(\"cols['2']\").alias('end'),\n",
    "    F.expr(\"cols['3']['entity']\").alias('entity'),\n",
    "    F.expr(\"cols['4']['resolved_text']\").alias('snomed_description'),\n",
    "    F.expr(\"cols['5']\").alias('snomed_code'),\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w6-BQ0MFL9Y"
   },
   "source": [
    "Light Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "executionInfo": {
     "elapsed": 1743,
     "status": "ok",
     "timestamp": 1601204600687,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LSukuO5eE1cZ",
    "outputId": "971bf816-2654-4514-9dd7-a0115bd8105e"
   },
   "outputs": [],
   "source": [
    "light_result[0]['resolution']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ER_SNOMED.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
