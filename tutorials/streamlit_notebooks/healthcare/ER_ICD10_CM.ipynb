{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA21Jo5d9SVq"
   },
   "source": [
    "\n",
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/ER_ICD10_CM.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzIdjHkAW8TB"
   },
   "source": [
    "# **ICD10-CM coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uDmeHEFW7_h"
   },
   "source": [
    "To run this yourself, you will need to upload your license keys to the notebook. Otherwise, you can look at the example outputs at the bottom of the notebook. To upload license keys, open the file explorer on the left side of the screen and upload `workshop_license_keys.json` to the folder that opens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIeCOiJNW-88"
   },
   "source": [
    "## 1. Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMIDv74CYN0d"
   },
   "source": [
    "Import license keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1601202970095,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "ttHPIV2JXbIM",
    "outputId": "96be4bf0-1dbd-47b4-e851-fe2dbb42e805"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/content/spark_nlp_for_healthcare.json', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n",
    "\n",
    "secret = license_keys['SECRET']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "sparknlp_version = license_keys[\"PUBLIC_VERSION\"]\n",
    "jsl_version = license_keys[\"JSL_VERSION\"]\n",
    "\n",
    "print ('SparkNLP Version:', sparknlp_version)\n",
    "print ('SparkNLP-JSL Version:', jsl_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQtc1CHaYQjU"
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 67550,
     "status": "ok",
     "timestamp": 1601203037867,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "CGJktFHdHL1n",
    "outputId": "fa9afae4-4520-4145-d404-34b3b3d471b4"
   },
   "outputs": [],
   "source": [
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==$sparknlp_version\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version --extra-index-url https://pypi.johnsnowlabs.com/$secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj5FRDV4YSXN"
   },
   "source": [
    "Import dependencies into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 65671,
     "status": "ok",
     "timestamp": 1601203037871,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "qUWyj8c6JSPP"
   },
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed6Htm7qDQB3"
   },
   "source": [
    "Start the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 82583,
     "status": "ok",
     "timestamp": 1601203058286,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "eaSM8-xhDRa4"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RgiqfX5XDqb"
   },
   "source": [
    "## 2. Select the Entity Resolver model and construct the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItrhGBVvXOXI"
   },
   "source": [
    "**NOTE: The mapping below is an example of how ICD10 resolvers work with different NER models. You can choose different combinations  according to your input data and requirements.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVKr8C2SrkZQ"
   },
   "source": [
    "\n",
    "\n",
    "Select the models:\n",
    "\n",
    "**ICD10 Entity Resolver models:**\n",
    "\n",
    "1.   **chunkresolve_icd10cm_clinical**\n",
    "2.   **chunkresolve_icd10cm_diseases_clinical**\n",
    "3.   **chunkresolve_icd10cm_injuries_clinical**\n",
    "4.   **chunkresolve_icd10cm_musculoskeletal_clinical**\n",
    "5.   **chunkresolve_icd10cm_neoplasms_clinical**\n",
    "6.   **chunkresolve_icd10cm_puerile_clinical**\n",
    "\n",
    "\n",
    "\n",
    "For more details: https://github.com/JohnSnowLabs/spark-nlp-models#pretrained-models---spark-nlp-for-healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1601206923557,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "cK9xxkkfrsLc"
   },
   "outputs": [],
   "source": [
    "#ner and entity resolver mapping\n",
    "ner_er_dict = {'chunkresolve_icd10cm_clinical': 'ner_clinical',\n",
    "              'chunkresolve_icd10cm_diseases_clinical': 'ner_diseases',\n",
    "              'chunkresolve_icd10cm_injuries_clinical': 'ner_clinical',\n",
    "              'chunkresolve_icd10cm_musculoskeletal_clinical': 'ner_clinical',\n",
    "              'chunkresolve_icd10cm_neoplasms_clinical': 'ner_bionlp',\n",
    "              'chunkresolve_icd10cm_puerile_clinical': 'ner_jsl'}\n",
    "# ER models are specfic to the codes they are trained on, so we need to filter out entities that will cause noise.\n",
    "wl_er_dict = {'chunkresolve_icd10cm_clinical': ['PROBLEM'],\n",
    "              'chunkresolve_icd10cm_diseases_clinical': ['Disease'],\n",
    "              'chunkresolve_icd10cm_injuries_clinical': ['PROBLEM'],\n",
    "              'chunkresolve_icd10cm_musculoskeletal_clinical': ['PROBLEM'],\n",
    "              'chunkresolve_icd10cm_neoplasms_clinical': ['CANCER','PATHOLOGICAL_FORMATION'],\n",
    "              'chunkresolve_icd10cm_puerile_clinical': ['PROBLEM']}\n",
    "\n",
    "# Change this to the model you want to use and re-run the cells below.\n",
    "model = 'chunkresolve_icd10cm_clinical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zweiG2ilZqoR"
   },
   "source": [
    "Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "executionInfo": {
     "elapsed": 157115,
     "status": "ok",
     "timestamp": 1601207082397,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LLuDz_t40be4",
    "outputId": "b0bcfef8-7fb0-43ab-8ec4-7e699d036e82"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences']) \\\n",
    "    .setOutputCol('tokens')\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = NerDLModel().pretrained(ner_er_dict[model], 'en', 'clinical/models')\\\n",
    "    .setInputCols(\"sentences\", \"tokens\", \"embeddings\")\\\n",
    "    .setOutputCol(\"ner_tags\")   \n",
    "\n",
    "#using defined whitelist. You can define your own as well.\n",
    "ner_chunker = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"ner_chunk\").setWhiteList(wl_er_dict[model])\n",
    "\n",
    "chunk_embeddings = ChunkEmbeddings()\\\n",
    "    .setInputCols(\"ner_chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"chunk_embeddings\")\n",
    "\n",
    "entity_resolver = \\\n",
    "    ChunkEntityResolverModel.pretrained(model,\"en\",\"clinical/models\")\\\n",
    "    .setInputCols(\"tokens\",\"chunk_embeddings\").setOutputCol(\"resolution\")\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_chunker,\n",
    "    chunk_embeddings,\n",
    "    entity_resolver])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "\n",
    "light_pipeline = sparknlp.base.LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y9GpdJhXIpD"
   },
   "source": [
    "## 3. Create example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1601207083673,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "vBOKkB2THdGI"
   },
   "outputs": [],
   "source": [
    "# Enter examples as strings in this array\n",
    "input_list = [\n",
    "\"\"\"The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion.\"\"\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gmrjqHSGcJx"
   },
   "source": [
    "# 4. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11313,
     "status": "ok",
     "timestamp": 1601207093715,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "xdhgKutMHUoC"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame({\"text\": input_list}))\n",
    "result = pipeline_model.transform(df)\n",
    "light_result = light_pipeline.fullAnnotate(input_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIVShVLhI68M"
   },
   "source": [
    "# 5. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472iBPpK-FvF"
   },
   "source": [
    "Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 20268,
     "status": "ok",
     "timestamp": 1601207102677,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "Qdh2BQaLI7tU",
    "outputId": "aabca254-f85d-410b-dbb9-d011c1db3ad7"
   },
   "outputs": [],
   "source": [
    "result.select(\n",
    "    F.explode(\n",
    "        F.arrays_zip('ner_chunk.result', \n",
    "                     'ner_chunk.begin',\n",
    "                     'ner_chunk.end',\n",
    "                     'ner_chunk.metadata',\n",
    "                     'resolution.metadata', 'resolution.result')\n",
    "    ).alias('cols')\n",
    ").select(\n",
    "    F.expr(\"cols['0']\").alias('chunk'),\n",
    "    F.expr(\"cols['1']\").alias('begin'),\n",
    "    F.expr(\"cols['2']\").alias('end'),\n",
    "    F.expr(\"cols['3']['entity']\").alias('entity'),\n",
    "    F.expr(\"cols['4']['resolved_text']\").alias('icd10_description'),\n",
    "    F.expr(\"cols['5']\").alias('icd10_code'),\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w6-BQ0MFL9Y"
   },
   "source": [
    "Light Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 20262,
     "status": "ok",
     "timestamp": 1601207102678,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LSukuO5eE1cZ",
    "outputId": "89cf0789-5a51-4301-c664-3ac1844c82fc"
   },
   "outputs": [],
   "source": [
    "light_result[0]['resolution']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ER_ICD10_CM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
