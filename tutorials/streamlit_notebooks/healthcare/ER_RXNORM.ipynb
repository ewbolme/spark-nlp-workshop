{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA21Jo5d9SVq"
   },
   "source": [
    "\n",
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/ER_RXNORM.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzIdjHkAW8TB"
   },
   "source": [
    "# **RxNorm coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uDmeHEFW7_h"
   },
   "source": [
    "To run this yourself, you will need to upload your license keys to the notebook. Otherwise, you can look at the example outputs at the bottom of the notebook. To upload license keys, open the file explorer on the left side of the screen and upload `workshop_license_keys.json` to the folder that opens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIeCOiJNW-88"
   },
   "source": [
    "## 1. Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMIDv74CYN0d"
   },
   "source": [
    "Import license keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1601204964894,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "ttHPIV2JXbIM",
    "outputId": "e2e60380-7e71-4cb2-c5bd-63ee6c2af10a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/content/spark_nlp_for_healthcare.json', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n",
    "\n",
    "secret = license_keys['SECRET']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "sparknlp_version = license_keys[\"PUBLIC_VERSION\"]\n",
    "jsl_version = license_keys[\"JSL_VERSION\"]\n",
    "\n",
    "print ('SparkNLP Version:', sparknlp_version)\n",
    "print ('SparkNLP-JSL Version:', jsl_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQtc1CHaYQjU"
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 78670,
     "status": "ok",
     "timestamp": 1601205044346,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "CGJktFHdHL1n",
    "outputId": "6917eee3-1eda-4919-8a51-f4b0432646d2"
   },
   "outputs": [],
   "source": [
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==$sparknlp_version\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version --extra-index-url https://pypi.johnsnowlabs.com/$secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj5FRDV4YSXN"
   },
   "source": [
    "Import dependencies into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 76566,
     "status": "ok",
     "timestamp": 1601205044348,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "qUWyj8c6JSPP"
   },
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed6Htm7qDQB3"
   },
   "source": [
    "Start the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97253,
     "status": "ok",
     "timestamp": 1601205066361,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "eaSM8-xhDRa4"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RgiqfX5XDqb"
   },
   "source": [
    "## 2. Select the Entity Resolver model and construct the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVKr8C2SrkZQ"
   },
   "source": [
    "Select the models:\n",
    "\n",
    "**RxNorm Entity Resolver models:**\n",
    "\n",
    "1.   **chunkresolve_rxnorm_cd_clinical**\n",
    "2.   **chunkresolve_rxnorm_sbd_clinical**\n",
    "3.   **chunkresolve_rxnorm_scd_clinical**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For more details: https://github.com/JohnSnowLabs/spark-nlp-models#pretrained-models---spark-nlp-for-healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1601205489347,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "cK9xxkkfrsLc"
   },
   "outputs": [],
   "source": [
    "## important: You can change NER models and whitelist entities according to your own requirements\n",
    "\n",
    "# ner and entity resolver mapping dict\n",
    "ner_er_dict = {'chunkresolve_rxnorm_scd_clinical': 'ner_posology',\n",
    "              'chunkresolve_rxnorm_cd_clinical': 'ner_posology',\n",
    "              'chunkresolve_rxnorm_sbd_clinical': 'ner_clinical'}\n",
    "# entities to whitelist, so resolver works on only required entities\n",
    "wl_er_dict = {'chunkresolve_rxnorm_scd_clinical': ['DRUG'],\n",
    "             'chunkresolve_rxnorm_cd_clinical': ['DRUG'],\n",
    "             'chunkresolve_rxnorm_sbd_clinical': ['TREATMENT']}\n",
    "# Change this to the model you want to use and re-run the cells below.\n",
    "model = 'chunkresolve_rxnorm_sbd_clinical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zweiG2ilZqoR"
   },
   "source": [
    "Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "executionInfo": {
     "elapsed": 128590,
     "status": "ok",
     "timestamp": 1601205620436,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LLuDz_t40be4",
    "outputId": "a2e97216-244f-418d-b1e5-52c67c6599ec"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences']) \\\n",
    "    .setOutputCol('tokens')\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = NerDLModel().pretrained(ner_er_dict[model], 'en', 'clinical/models')\\\n",
    "    .setInputCols(\"sentences\", \"tokens\", \"embeddings\")\\\n",
    "    .setOutputCol(\"ner_tags\")   \n",
    "\n",
    "ner_chunker = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"ner_chunk\").setWhiteList(wl_er_dict[model])\n",
    "\n",
    "chunk_embeddings = ChunkEmbeddings()\\\n",
    "    .setInputCols(\"ner_chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"chunk_embeddings\")\n",
    "\n",
    "entity_resolver = \\\n",
    "    ChunkEntityResolverModel.pretrained(model,\"en\",\"clinical/models\")\\\n",
    "    .setInputCols(\"tokens\",\"chunk_embeddings\").setOutputCol(\"resolution\")\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_chunker,\n",
    "    chunk_embeddings,\n",
    "    entity_resolver])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "\n",
    "light_pipeline = sparknlp.base.LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y9GpdJhXIpD"
   },
   "source": [
    "## 3. Create example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1601205726523,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "vBOKkB2THdGI"
   },
   "outputs": [],
   "source": [
    "# Enter examples as strings in this array\n",
    "input_list = [\n",
    "\"\"\"The patient is a 40-year-old white male who presents with a chief complaint of \"chest pain\". The patient is diabetic and has a prior history of coronary artery disease. The patient presents today stating that his chest pain started yesterday evening and has been somewhat intermittent. He has been advised Aspirin 81 milligrams QDay. Humulin N. insulin 50 units in a.m. HCTZ 50 mg QDay. Nitroglycerin 1/150 sublingually PRN chest pain.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gmrjqHSGcJx"
   },
   "source": [
    "# 4. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10826,
     "status": "ok",
     "timestamp": 1601205738514,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "xdhgKutMHUoC"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame({\"text\": input_list}))\n",
    "result = pipeline_model.transform(df)\n",
    "light_result = light_pipeline.fullAnnotate(input_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIVShVLhI68M"
   },
   "source": [
    "# 5. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472iBPpK-FvF"
   },
   "source": [
    "Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 18650,
     "status": "ok",
     "timestamp": 1601205748420,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "Qdh2BQaLI7tU",
    "outputId": "86ed0ea7-72f2-40e8-93fc-809a33609d54"
   },
   "outputs": [],
   "source": [
    "result.select(\n",
    "    F.explode(\n",
    "        F.arrays_zip('ner_chunk.result', \n",
    "                     'ner_chunk.begin',\n",
    "                     'ner_chunk.end',\n",
    "                     'ner_chunk.metadata',\n",
    "                     'resolution.metadata', 'resolution.result')\n",
    "    ).alias('cols')\n",
    ").select(\n",
    "    F.expr(\"cols['0']\").alias('chunk'),\n",
    "    F.expr(\"cols['1']\").alias('begin'),\n",
    "    F.expr(\"cols['2']\").alias('end'),\n",
    "    F.expr(\"cols['3']['entity']\").alias('entity'),\n",
    "    F.expr(\"cols['4']['resolved_text']\").alias('RxNorm_description'),\n",
    "    F.expr(\"cols['5']\").alias('RxNorm_code'),\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w6-BQ0MFL9Y"
   },
   "source": [
    "Light Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 15467,
     "status": "ok",
     "timestamp": 1601205748421,
     "user": {
      "displayName": "Hasham Ul Haq",
      "photoUrl": "",
      "userId": "10508284328555930330"
     },
     "user_tz": -300
    },
    "id": "LSukuO5eE1cZ",
    "outputId": "2d864f68-2216-4125-d558-7afd0419efea"
   },
   "outputs": [],
   "source": [
    "light_result[0]['resolution']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ER_RXNORM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
