{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EotGy802StAn"
   },
   "source": [
    "\n",
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/CLASSIFICATION_MULTILABEL_TOXIC.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZGjocpuSwmn"
   },
   "source": [
    "# **Detect toxic content in comments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYdYPhkSmMTE"
   },
   "source": [
    "## 1. Colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "izRnq4e08XKs",
    "outputId": "c40de2e8-5357-4e8f-b1fa-58289409e612"
   },
   "outputs": [],
   "source": [
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "! pip install --ignore-installed -q spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxs7u_Xo7-i9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = sparknlp.start(gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-53R49pmSBN"
   },
   "source": [
    "## 2. Pipeline creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Cw1BGrmGTo"
   },
   "source": [
    "Create pipeline to be trained on example inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "DYVAXYXW7-jf",
    "outputId": "7a78d7c0-d679-47b8-ec6a-5dab9cf7f8a5"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddingsSentence = UniversalSentenceEncoder.pretrained(lang='en') \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "classsifierdl = MultiClassifierDLApproach() \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"class\") \\\n",
    "    .setLabelColumn(\"labels\") \\\n",
    "    .setMaxEpochs(10) \\\n",
    "    .setLr(1e-3) \\\n",
    "    .setThreshold(0.7) \\\n",
    "    .setValidationSplit(0.2) \\\n",
    "    .setOutputLogsPath('./') \\\n",
    "    .setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    embeddingsSentence,\n",
    "    classsifierdl\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRQRt36fmA9_"
   },
   "source": [
    "Download training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "dsBntyOYS3T9",
    "outputId": "19e2da9d-7fc1-43dc-c21d-853ae5839bf9"
   },
   "outputs": [],
   "source": [
    "! curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_train.snappy.parquet'\n",
    "! curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_test.snappy.parquet'\n",
    "trainDataset = spark.read.parquet(\"toxic_train.snappy.parquet\")\n",
    "testDataset = spark.read.parquet(\"toxic_test.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFiOVvswSqcC"
   },
   "outputs": [],
   "source": [
    "res = trainDataset.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh1h5WJMSqcE",
    "outputId": "00750c89-0efd-43eb-ed8b-1bbec14bc6f8"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxoKXXmPSqcH",
    "outputId": "796a7a42-0cd1-43ed-8e78-30e0cee070f1"
   },
   "outputs": [],
   "source": [
    "all_labs = []\n",
    "for r in res['labels'].values:\n",
    "    all_labs.extend(r)\n",
    "set(all_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIIF2e1Kl4fF"
   },
   "source": [
    "Train the pipeline model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlGbDPbjRQ_E"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fm-0aY6Gmppl"
   },
   "source": [
    "Example training log:\n",
    "\n",
    "\n",
    "```\n",
    "Training started - epochs: 10 - learning_rate: 0.001 - batch_size: 64 - training_examples: 127657 - classes: 7\n",
    "Epoch 0/10 - 46.00s - loss: 0.079907365 - acc: 0.9723665 - val_loss: 0.07093345 - val_acc: 0.97352755 - val_f1: 0.91585624 - val_tpr: 0.9008827 - batches: 1995\n",
    "Epoch 1/10 - 28.26s - loss: 0.06964213 - acc: 0.9747156 - val_loss: 0.069492534 - val_acc: 0.9739863 - val_f1: 0.91734964 - val_tpr: 0.9029063 - batches: 1995\n",
    "Epoch 2/10 - 27.99s - loss: 0.06808146 - acc: 0.9752242 - val_loss: 0.06841504 - val_acc: 0.97444665 - val_f1: 0.9187962 - val_tpr: 0.9039833 - batches: 1995\n",
    "Epoch 3/10 - 27.94s - loss: 0.066884466 - acc: 0.9757066 - val_loss: 0.06769186 - val_acc: 0.9746787 - val_f1: 0.9195223 - val_tpr: 0.90438014 - batches: 1995\n",
    "Epoch 4/10 - 28.04s - loss: 0.06587076 - acc: 0.9761073 - val_loss: 0.067252316 - val_acc: 0.9749198 - val_f1: 0.92028916 - val_tpr: 0.9049118 - batches: 1995\n",
    "Epoch 5/10 - 28.05s - loss: 0.06501821 - acc: 0.97637606 - val_loss: 0.06700255 - val_acc: 0.9750182 - val_f1: 0.9205762 - val_tpr: 0.90496385 - batches: 1995\n",
    "Epoch 6/10 - 27.92s - loss: 0.064287946 - acc: 0.9765889 - val_loss: 0.06686394 - val_acc: 0.9750045 - val_f1: 0.9205367 - val_tpr: 0.904986 - batches: 1995\n",
    "Epoch 7/10 - 28.19s - loss: 0.063645855 - acc: 0.97682655 - val_loss: 0.0667824 - val_acc: 0.97498673 - val_f1: 0.92048156 - val_tpr: 0.90499973 - batches: 1995\n",
    "Epoch 8/10 - 28.53s - loss: 0.06306508 - acc: 0.97704613 - val_loss: 0.06673989 - val_acc: 0.974991 - val_f1: 0.9205486 - val_tpr: 0.9056079 - batches: 1995\n",
    "Epoch 9/10 - 28.61s - loss: 0.0625258 - acc: 0.97720623 - val_loss: 0.06672904 - val_acc: 0.9750411 - val_f1: 0.9207759 - val_tpr: 0.90621996 - batches: 1995\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pD1oGGgwmVUe"
   },
   "source": [
    "## 3. Testing and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzia33pWl0Lc"
   },
   "source": [
    "Run the model on the test dataset to evaluate performance and generate examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUo4_e2s7-js"
   },
   "outputs": [],
   "source": [
    "test_res = pipelineModel.transform(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFCxC-YwlobE"
   },
   "source": [
    "Visualizing the raw test dataset after classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "F9cTSIObV0_C",
    "outputId": "316893d7-9e97-48c7-e9ab-c2cb8f33bee6"
   },
   "outputs": [],
   "source": [
    "test_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlyXUWWulijW"
   },
   "source": [
    "Write some sample inputs and outputs from the test dataset to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_rcLmhnWfYS"
   },
   "outputs": [],
   "source": [
    "! mkdir -p inputs\n",
    "! mkdir -p outputs\n",
    "\n",
    "result = test_res.toPandas()\n",
    "for i in range(1, 11):\n",
    "    text = result[['document']].iloc[i][0][0].result\n",
    "    with open(f'inputs/{i}.txt', 'w') as f:\n",
    "        f.write(text[:96].replace('\\n', '') + \" ...\\n\" + text)\n",
    "    result[['class']].iloc[i].to_json(f'outputs/{i}.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CLASSIFICATION_MULTILABEL_TOXIC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
