{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQFg6BDHKcc9"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/KEYPHRASE_EXTRACTION.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOqOdIKkECq1"
   },
   "source": [
    "# **Extract keyphrases from documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99Qqhw7sEFyx"
   },
   "source": [
    "You can look at the example outputs stored at the bottom of the notebook to see what the model can do, or enter your own inputs to transform in the \"Inputs\" section. Find more about this keyphrase extraction model in another notebook [here](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/9.Keyword_Extraction_YAKE.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mX0gNZv8MRtQ"
   },
   "source": [
    "## 1. Colab setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuXQkGilKWu7"
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "C-Nz_2A8Jos8",
    "outputId": "060f69dd-4a8d-4662-e2e9-5753bdc8ae53"
   },
   "outputs": [],
   "source": [
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "! pip install --ignore-installed -q spark-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vb1y9TT8Ke_U"
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGMK0q_IIO_I"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "# Import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Import SparkNLP\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "# Start Spark session\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYDxs7_CMtpf"
   },
   "source": [
    "## 2. Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLk2XcmDMwTI"
   },
   "source": [
    "Enter inputs as strings in this list. Later cells of the notebook will extract keyphrases from whatever inputs are entered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_L-3nRuM0d4"
   },
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    \"\"\"Extracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted.\"\"\",\n",
    "    \"\"\"Iodine deficiency is a lack of the trace element iodine, an essential nutrient in the diet. It may result in metabolic problems such as goiter, sometimes as an endemic goiter as well as cretinism due to untreated congenital hypothyroidism, which results in developmental delays and other health problems. Iodine deficiency is an important global health issue, especially for fertile and pregnant women. It is also a preventable cause of intellectual disability.\n",
    "\n",
    "Iodine is an essential dietary mineral for neurodevelopment among offsprings and toddlers. The thyroid hormones thyroxine and triiodothyronine contain iodine. In areas where there is little iodine in the diet, typically remote inland areas where no marine foods are eaten, iodine deficiency is common. It is also common in mountainous regions of the world where food is grown in iodine-poor soil.\n",
    "\n",
    "Prevention includes adding small amounts of iodine to table salt, a product known as iodized salt. Iodine compounds have also been added to other foodstuffs, such as flour, water and milk, in areas of deficiency. Seafood is also a well known source of iodine.\"\"\",\n",
    "    \"\"\"The Prague Quadrennial of Performance Design and Space was established in 1967 to bring the best of design for performance, scenography, and theatre architecture to the front line of cultural activities to be experienced by professional and emerging artists as well as the general public. The quadrennial exhibitions, festivals, and educational programs act as a global catalyst of creative progress by encouraging experimentation, networking, innovation, and future collaborations. PQ aims to honor, empower and celebrate the work of designers, artists and architects while inspiring and educating audiences, who are the most essential element of any live performance. The Prague Quadrennial strives to present performance design as an art form concerned with creation of active performance environments, that are far beyond merely decorative or beautiful, but emotionally charged, where design can become a quest, a question, an argument, a threat, a resolution, an agent of change, or a provocation. Performance design is a collaborative field where designers mix, fuse and blur the lines between multiple artistic disciplines to search for new approaches and new visions.\n",
    "\n",
    "The Prague Quadrennial organizes an expansive program of international projects and activities between the main quadrennial events â€“ performances, exhibitions, symposia, workshops, residencies, and educational initiatives serve as an international platform for exploring the practice, theory and education of contemporary performance design in the most encompassing terms.\"\"\",\n",
    "    \"\"\"Author Nathan Wiseman-Trowse explained that the \"approach to the sheer physicality of sound\" integral to dream pop was \"arguably pioneered in popular music by figures such as Phil Spector and Brian Wilson\". The music of the Velvet Underground in the 1960s and 1970s, which experimented with repetition, tone, and texture over conventional song structure, was also an important touchstone in the genre's development George Harrison's 1970 album All Things Must Pass, with its Spector-produced Wall of Sound and fluid arrangements, led music journalist John Bergstrom to credit it as a progenitor of the genre.\n",
    "\n",
    "Reynolds described dream pop bands as \"a wave of hazy neo-psychedelic groups\", noting the influence of the \"ethereal soundscapes\" of bands such as Cocteau Twins. Rolling Stone's Kory Grow described \"modern dream pop\" as originating with the early 1980s work of Cocteau Twins and their contemporaries, while PopMatters' AJ Ramirez noted an evolutionary line from gothic rock to dream pop. Grow considered Julee Cruise's 1989 album Floating into the Night, written and produced by David Lynch and Angelo Badalamenti, as a significant development of the dream pop sound which \"gave the genre its synthy sheen.\" The influence of Cocteau Twins extended to the expansion of the genre's influence into Cantopop and Mandopop through the music of Faye Wong, who covered multiple Cocteau Twins songs, including tracks featured in Chungking Express, in which she also acted. Cocteau Twins would go on to collaborate with Wong on original songs of hers, and Wong contributed vocals to a limited release of a late Cocteau Twins single.\n",
    "\n",
    "In the early 1990s, some dream pop acts influenced by My Bloody Valentine, such as Seefeel, were drawn to techno and began utilizing elements such as samples and sequenced rhythms. Ambient pop music was described by AllMusic as \"essentially an extension of the dream pop that emerged in the wake of the shoegazer movement\", distinct for its incorporation of electronic textures.\n",
    "\n",
    "Much of the music associated with the 2009-coined term \"chillwave\" could be considered dream pop. In the opinion of Grantland's David Schilling, when \"chillwave\" was popularized, the discussion that followed among music journalists and bloggers revealed that labels such as \"shoegaze\" and \"dream pop\" were ultimately \"arbitrary and meaningless\".\"\"\",\n",
    "    \"\"\"North Ingria was located in the Karelian Isthmus, between Finland and Soviet Russia. It was established 23 January 1919. The republic was first served by a post office at the Rautu railway station on the Finnish side of the border. As the access across the border was mainly restricted, the North Ingrian postal service was finally launched in the early 1920. The man behind the idea was the lieutenant colonel Georg Elfvengren, head of the governing council of North Ingria. He was also known as an enthusiastic stamp collector. The post office was opened at the capital village of Kirjasalo.\n",
    "\n",
    "The first series of North Ingrian stamps were issued in 21 March 1920. They were based on the 1917 Finnish \"Model Saarinen\" series, a stamp designed by the Finnish architect Eliel Saarinen. The first series were soon sold to collectors, as the postage stamps became the major financial source of the North Ingrian government. The second series was designed for the North Ingrian postal service and issued 2 August 1920. The value of both series was in Finnish marks and similar to the postal fees of Finland. The number of letters sent from North Ingria was about 50 per day, most of them were carried to Finland. They were mainly sent by the personnel of the Finnish occupying forces. Large number of letters were also sent in pure philatelic purposes.\n",
    "\n",
    "With the Treaty of Tartu, the area was re-integrated into Soviet Russia and the use of the North Ingrian postage stamps ended in 4 December 1920. Stamps were still sold in Finland in 1921 with an overprinting \"Inkerin hyvÃ¤ksi\" (For the Ingria), but they were no longer valid. Funds of the sale went for the North Ingrian refugees.\"\"\"\n",
    "]\n",
    "\n",
    "# Change these to wherever you want your inputs and outputs to go\n",
    "INPUT_FILE_PATH = \"inputs\"\n",
    "OUTPUT_FILE_PATH = \"outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF1kBJ9vA9kx"
   },
   "source": [
    "Write the example inputs to the input folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UITSel_Yr4IC"
   },
   "outputs": [],
   "source": [
    "! mkdir -p $INPUT_FILE_PATH\n",
    "\n",
    "for i, text in enumerate(input_list):\n",
    "    open(f'{INPUT_FILE_PATH}/Example{i + 1}.txt', 'w') \\\n",
    "        .write(text[:min(len(text) - 10, 100)] + '... \\n' + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0zS5R_7MV7T"
   },
   "source": [
    "## 3. Pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8z7wW-eIMoN2"
   },
   "source": [
    "Create the NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpIz2L-bIO_Y"
   },
   "outputs": [],
   "source": [
    "# Transforms the raw text into a document readable by the later stages of the\n",
    "# pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "# Separates the document into sentences\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('sentences')# \\\n",
    "    #.setDetectLists(True)\n",
    "\n",
    "# Separates sentences into individial tokens (words)\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentences']) \\\n",
    "    .setOutputCol('tokens') \\\n",
    "    .setContextChars(['(', ')', '?', '!', '.', ','])\n",
    "\n",
    "# The keyphrase extraction model. Change MinNGrams and MaxNGrams to set the\n",
    "# minimum and maximum length of possible keyphrases, and change NKeywords to\n",
    "# set the amount of potential keyphrases identified per document.\n",
    "keywords = YakeModel() \\\n",
    "    .setInputCols('tokens') \\\n",
    "    .setOutputCol('keywords') \\\n",
    "    .setMinNGrams(2) \\\n",
    "    .setMaxNGrams(5) \\\n",
    "    .setNKeywords(100) \\\n",
    "    .setStopWords(StopWordsCleaner().getStopWords())\n",
    "\n",
    "# Assemble all of these stages into a pipeline, then fit the pipeline on an\n",
    "# empty data frame so it can be used to transform new inputs.\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    keywords\n",
    "])\n",
    "empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "\n",
    "# LightPipeline is faster than Pipeline for small datasets\n",
    "light_pipeline = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZsaT1M_Mapv"
   },
   "source": [
    "## 4. Output creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InE4U2Fq-ih4"
   },
   "source": [
    "Utility functions to create more useful sets of keyphrases from the raw data frame produced by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xR7dJVA53qKK"
   },
   "outputs": [],
   "source": [
    "def adjusted_score(row, pow=2.5):\n",
    "    \"\"\"This function adjusts the scores of potential key phrases to give better\n",
    "    scores to phrases with more words (which will naturally have worse scores\n",
    "    due to the nature of the model). You can change the exponent to reward\n",
    "    longer phrases more or less. Higher exponents reward longer phrases.\"\"\"\n",
    "    return ((row.result.count(' ') + 1) ** pow /\n",
    "            (float(row.metadata['score']) + 0.1))\n",
    "\n",
    "def get_top_ranges(phrases, input_text):\n",
    "    \"\"\"Combine phrases that overlap.\"\"\"\n",
    "    starts = sorted([row['begin'] for row in phrases])\n",
    "    ends = sorted([row['end'] for row in phrases])\n",
    "\n",
    "    ranges = [[starts[0], None]]\n",
    "    for i in range(len(starts) - 1):\n",
    "        if ends[i] < starts[i + 1]:\n",
    "            ranges[-1][1] = ends[i]\n",
    "            ranges.append([starts[i + 1], None])\n",
    "    ranges[-1][1] = ends[-1]\n",
    "    return [{\n",
    "        'begin': range[0],\n",
    "        'end': range[1],\n",
    "        'phrase': input_text[range[0]:range[1] + 1]\n",
    "     } for range in ranges]\n",
    "\n",
    "def remove_duplicates(phrases):\n",
    "    \"\"\"Remove phrases that appear multiple times.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(phrases):\n",
    "        j = i + 1\n",
    "        while j < len(phrases):\n",
    "            if phrases[i]['phrase'] == phrases[j]['phrase']:\n",
    "                phrases.remove(phrases[j])\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    return phrases\n",
    "\n",
    "def get_output_lists(df_row):\n",
    "    \"\"\"Returns a tuple with two lists of five phrases each. The first combines\n",
    "    key phrases that overlap to create longer kep phrases, which is best for\n",
    "    highlighting key phrases in text, and the seocnd is simply the keyphrases\n",
    "    with the highest scores, which is best for summarizing a document.\"\"\"\n",
    "    keyphrases = []\n",
    "    for row in df_row.keywords:\n",
    "        keyphrases.append({\n",
    "            'begin': row.begin,\n",
    "            'end': row.end,\n",
    "            'phrase': row.result,\n",
    "            'score': adjusted_score(row)\n",
    "        })\n",
    "    keyphrases = sorted(keyphrases, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    return (\n",
    "        get_top_ranges(keyphrases[:20], df_row.text)[:5],\n",
    "        remove_duplicates(keyphrases[:10])[:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9EswUu9-vPo"
   },
   "source": [
    "Transform the example inputs to create a data frame storing the identified keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YAV9JFfIO_f"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame({'text': input_list}))\n",
    "result = light_pipeline.transform(df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPQG2x_--3Ik"
   },
   "source": [
    "For each example, create two JSON files containing selections of the best keyphrases for the document. See the docstring of `get_output_lists` two cells above to learn more about the two JSON files produced. These JSON files are used directly in the public demo app for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-y9D7PRCiS8"
   },
   "outputs": [],
   "source": [
    "! mkdir -p $OUTPUT_FILE_PATH\n",
    "\n",
    "for i in range(len(result)):\n",
    "    top_ranges, top_summaries = get_output_lists(result.iloc[i])\n",
    "    with open(f'{OUTPUT_FILE_PATH}/Example{i + 1}.json', 'w') as ranges_file:\n",
    "        json.dump(top_ranges, ranges_file)\n",
    "    with open(f'{OUTPUT_FILE_PATH}/Example{i + 1}_summaries.json', 'w') \\\n",
    "            as summaries_file:\n",
    "        json.dump(top_summaries, summaries_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIJtDyaRI6hh"
   },
   "source": [
    "## 5. Visualize outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaZpx6SaJCBN"
   },
   "source": [
    "The raw pandas data frame containing the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "mzgZwqv9I6Cl",
    "outputId": "a34750d9-9737-4f95-d4a0-b02ba8a2e27b"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EbaXFSRJX8s"
   },
   "source": [
    "The list of the top keyphrases (with overlapping keyphrases merged) for the last example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "a5j9LTWLJI_L",
    "outputId": "1ae972f6-eff3-4723-8904-2b7267000d22"
   },
   "outputs": [],
   "source": [
    "top_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3SrNEDaOJl6V"
   },
   "source": [
    "The list of the best summary kephrases (with duplicates removed) for the last example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "C0I68Le1JM_8",
    "outputId": "5d437553-050f-4ccd-8a54-63cbaf69efd2"
   },
   "outputs": [],
   "source": [
    "top_summaries"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KEYPHRASE_EXTRACTION.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
